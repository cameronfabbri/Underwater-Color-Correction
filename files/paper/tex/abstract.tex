\begin{abstract}
Autonomous underwater vehicles (AUVs) rely on a variety of sensors -- acoustic, inertial and visual -- for intelligent decision 
making. Due to its non-intrusive, passive nature, and high information content, vision is an attractive sensing modality, 
particularly at shallower depths. However, factors such as light refraction and absorption, suspended particles in the water, and 
color distortion affect the quality of visual data, resulting in noisy and distorted images. AUVs that rely on visual sensing thus 
face difficult challenges, and consequently exhibit poor performance on vision-driven tasks. This paper proposes a method to 
improve the quality of visual underwater scenes using Generative Adversarial Networks (GANs), with the goal of improving input to 
vision-driven behaviors further down the autonomy pipeline. Furthermore, we show how recently proposed methods are able to 
generate a dataset for the purpose of such underwater image restoration. For any visually-guided underwater robots, this 
improvement can result in increased safety and reliability through robust visual perception. To that effect, we present 
quantitative and qualitative data which demonstrates that images corrected through the proposed approach generate more visually 
appealing images, and also provide increased accuracy for a diver tracking algorithm.
\end{abstract}

\nostarnote{Reconstruction meaning what we are doing here, 
right? So this is a contribution too, to show how CycleGAN is used to generate the dataset, correct?}